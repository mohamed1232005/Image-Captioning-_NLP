# Image-Captioning_NLP


This project implements and compares multiple deep learning models for automatic image caption generation using the **Flickr8k** dataset. The core idea is to extract visual features using a ResNet152V2 CNN and generate meaningful captions using various language decoders (LSTM, GRU, BART, GPT-2).

## üìå Project Goals

- Extract high-quality visual features using **ResNet152V2**
- Implement multiple decoders to generate captions:
  - LSTM
  - GRU
  - BART
  - GPT-2
- Compare their performance using BLEU score
- Build a user-friendly **Flask web interface** for real-time image captioning

## üóÇÔ∏è Dataset

- **Flickr8k**: 8,000 images with 5 captions each
- Source: [Kaggle - Flickr8k](https://www.kaggle.com/datasets/adityajn105/flickr8k)
- Size: ~1.1 GB (Images + `captions.txt`)
- We used the 8k version (not 30k) for faster training and comparison across multiple models.

## üß† Model Architectures

### üì∑ Feature Extractor
- **ResNet152V2 (Pre-trained)** used for all models
- Output: 2048-dim feature vector per image

### üó£Ô∏è Caption Generators (Decoders)
| Decoder | Architecture |
|--------|--------------|
| LSTM   | Dense ‚Üí LSTM ‚Üí Dense Softmax |
| GRU    | Dense ‚Üí GRU ‚Üí Dense Softmax |
| BART   | VisionEncoderDecoderModel from Hugging Face |
| GPT-2  | VisionEncoderDecoderModel with GPT-2 decoder |

## üß™ Evaluation

We used **BLEU Score** to evaluate generated captions against reference captions.
- BLEU is a precision-based metric that checks word overlaps.
- Note: Higher BLEU may not always reflect more human-like descriptions.

| Model | BLEU Score |
|-------|------------|
| GPT-2 | 0.01722    |
| BART  | 0.015113   |
| LSTM  | 9.03e-155  |
| GRU   | 0.041      |

Although GRU had the highest BLEU, **BART** generated the most fluent and meaningful captions in practice.

## üåê Deployment

- Implemented using **Flask API**
- Web interface allows image upload
- Captions generated by **two models** in real-time (e.g., ViT-GPT2 and BLIP)
- BLEU comparison displayed dynamically

## üìä Comparison with Related Work

| Criterion | Previous Work (VGG16 + LSTM) | This Work (ResNet152V2 + Multiple Decoders) |
|----------|-------------------------------|---------------------------------------------|
| Feature Extractor | VGG16 (shallow) | ResNet152V2 (deep, robust) |
| Decoder | LSTM only | LSTM, GRU, BART, GPT-2 |
| Architecture | No attention or transformers | Transformers + modern decoders |
| Evaluation | BLEU 0.27 (single) | BLEU across 4 models |
| Deployment | Not implemented | Fully deployed (Flask) |
| Limitation Addressed | Weak language structure | High fluency with transformers |

## üõ†Ô∏è Requirements

- Python 3.8+
- TensorFlow / PyTorch
- Hugging Face Transformers
- Flask
- OpenCV, PIL, NLTK

```bash
pip install -r requirements.txt
